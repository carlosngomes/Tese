{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_file(filename):\n",
    "    loaded_dict = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(':')\n",
    "            subject = parts[0].strip()\n",
    "            events = [event.strip('][').split('][') for event in parts[1:]]\n",
    "            # Split each feature and convert to float\n",
    "            events = [[float(feature) for feature in event.split(',')] for sublist in events for event in sublist]\n",
    "            loaded_dict[subject] = events\n",
    "    return loaded_dict\n",
    "\n",
    "def extend_list(dict):\n",
    "    all_subj=[]\n",
    "    for subject_id in dict.keys():\n",
    "        all_subj.extend(dict[subject_id])\n",
    "    return all_subj\n",
    "\n",
    "def mean_of_lists(input_list):\n",
    "    return [sum(items) / len(items) for items in zip(*input_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "with open('labels.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(':')\n",
    "        subject = parts[0].strip()\n",
    "        data = parts[1].strip().split('][')\n",
    "        data = [x.strip('[').strip(']').split(',') for x in data]\n",
    "        data = [int(x) for sublist in data for x in sublist]\n",
    "        labels[subject] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_features=load_dict_from_file('theta_features.txt')\n",
    "frequency_features=load_dict_from_file('frequency_features.txt')\n",
    "temporal_features=load_dict_from_file('temporal_features.txt')\n",
    "all_features=load_dict_from_file('all_features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(kern,feature, labels):\n",
    "    num_iterations = 100\n",
    "    test_size = 0.3  \n",
    "    train_size= 0.7\n",
    "\n",
    "    model = SVC(kernel=kern, class_weight='balanced') #weighted SVM with rbf kernel\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    balanced_accuracies = []\n",
    "    coefs=[]\n",
    "    value= np.array(extend_list(feature))\n",
    "    label= np.array(extend_list(labels))\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        shuffle_split = StratifiedShuffleSplit(test_size=test_size, train_size=train_size) #Monte-Carlo 100 iterations, 70% train, 30% test\n",
    "        \n",
    "        iteration_scores = []\n",
    "\n",
    "        for train_index, test_index in shuffle_split.split(value,label):\n",
    "            X_train, X_test = value[train_index], value[test_index]\n",
    "            y_train, y_test = label[train_index], label[test_index]\n",
    "            \n",
    "            X_train_scaled = scaler.fit_transform(X_train) #Feature standardization\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            score = model.score(X_test_scaled, y_test)\n",
    "            iteration_scores.append(score)\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn)\n",
    "            \n",
    "            specificity = tn / (tn + fp)\n",
    "            \n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            \n",
    "            sensitivities.append(sensitivity)\n",
    "            specificities.append(specificity)\n",
    "            balanced_accuracies.append(balanced_accuracy)\n",
    "            if kern=='linear':\n",
    "                coef = model.coef_[0]\n",
    "                coefs.append(coef)\n",
    "            else: \n",
    "                continue\n",
    "\n",
    "\n",
    "        mean_score = np.mean(iteration_scores)\n",
    "        std_score = np.std(iteration_scores)\n",
    "\n",
    "        print(\"Mean score: {:.4f}\".format(mean_score))\n",
    "        print(\"Std score: {:.4f}\".format(std_score))\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "        mean_scores.append(mean_score)\n",
    "        std_scores.append(std_score)\n",
    "\n",
    "    overall_mean_score = np.mean(mean_scores)\n",
    "    overall_std_score = np.mean(std_scores)\n",
    "    print(\"Overall Mean score: {:.4f}\".format(overall_mean_score))\n",
    "    print(\"Overall Std score: {:.4f}\".format(overall_std_score))\n",
    "\n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "    mean_specificity = np.mean(specificities)\n",
    "    std_specificity = np.std(specificities)\n",
    "    mean_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    std_balanced_accuracy = np.std(balanced_accuracies)\n",
    "    mean_coef= mean_of_lists(coefs)\n",
    "    print(\"Mean Sensitivity: {:.4f}\".format(mean_sensitivity))\n",
    "    print(\"Std Sensitivity: {:.4f}\".format(std_sensitivity))\n",
    "    print(\"Mean Specificity: {:.4f}\".format(mean_specificity))\n",
    "    print(\"Std Specificity: {:.4f}\".format(std_specificity))\n",
    "    print(\"Mean Balanced Accuracy: {:.4f}\".format(mean_balanced_accuracy))\n",
    "    print(\"Std Balanced Accuracy: {:.4f}\".format(std_balanced_accuracy))  \n",
    "    if kern=='linear':\n",
    "        for feature, score in enumerate(mean_coef):\n",
    "            print(f\"Feature {feature+1}: {score}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.6291\n",
      "Std score: 0.0448\n",
      "-------------------------------\n",
      "Mean score: 0.6399\n",
      "Std score: 0.0371\n",
      "-------------------------------\n",
      "Mean score: 0.6449\n",
      "Std score: 0.0290\n",
      "-------------------------------\n",
      "Mean score: 0.6387\n",
      "Std score: 0.0313\n",
      "-------------------------------\n",
      "Mean score: 0.6404\n",
      "Std score: 0.0421\n",
      "-------------------------------\n",
      "Mean score: 0.6443\n",
      "Std score: 0.0138\n",
      "-------------------------------\n",
      "Mean score: 0.6534\n",
      "Std score: 0.0402\n",
      "-------------------------------\n",
      "Mean score: 0.6372\n",
      "Std score: 0.0405\n",
      "-------------------------------\n",
      "Mean score: 0.6368\n",
      "Std score: 0.0388\n",
      "-------------------------------\n",
      "Mean score: 0.6162\n",
      "Std score: 0.0384\n",
      "-------------------------------\n",
      "Mean score: 0.6347\n",
      "Std score: 0.0248\n",
      "-------------------------------\n",
      "Mean score: 0.6307\n",
      "Std score: 0.0287\n",
      "-------------------------------\n",
      "Mean score: 0.6454\n",
      "Std score: 0.0475\n",
      "-------------------------------\n",
      "Mean score: 0.6145\n",
      "Std score: 0.0332\n",
      "-------------------------------\n",
      "Mean score: 0.6474\n",
      "Std score: 0.0277\n",
      "-------------------------------\n",
      "Mean score: 0.6435\n",
      "Std score: 0.0321\n",
      "-------------------------------\n",
      "Mean score: 0.6540\n",
      "Std score: 0.0387\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "classification('linear',frequency_features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
