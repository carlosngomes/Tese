{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features:\n",
      "Total: 1549\n",
      "Pre: 804\n",
      "Post: 806\n",
      "Theta Features:\n",
      "Total: 587\n",
      "Pre: 324\n",
      "Post: 324\n",
      "Frequency Features:\n",
      "Total: 1307\n",
      "Pre: 684\n",
      "Post: 684\n",
      "Temporal Features:\n",
      "Total: 242\n",
      "Pre: 120\n",
      "Post: 122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.functions as src\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features = \"dataset2_features.csv\"\n",
    "features = pd.read_csv(features, sep=',', header=0, index_col=None)\n",
    "Y = features[['labels']]\n",
    "\n",
    "X_all = features.drop('labels', axis=1)\n",
    "all_pre= ['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_','25_','26_','27_','28_','29_','30_','33_','34_']\n",
    "X_all_pre=X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in all_pre)]]\n",
    "all_post=  ['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_','19_','20_','21_','22_','23_','24_','31_','32_','35_','36_']\n",
    "X_all_post=X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in all_post)]]\n",
    "print('All Features:')\n",
    "print('Total: ' + str(len(X_all.columns))) \n",
    "print('Pre: ' + str(len(X_all_pre.columns)))\n",
    "print('Post: ' + str(len(X_all_post.columns)))\n",
    "\n",
    "theta_prefixes= ['1_','2_','3_','4_','5_','6_','7_','8_','9_','10_','11_','12_','13_','14_','15_','16_','17_','18_']\n",
    "X_theta= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in theta_prefixes)]]\n",
    "theta_pre= ['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_']\n",
    "X_theta_pre= X_theta[[col for col in X_theta.columns if any(col.startswith(prefix) for prefix in theta_pre)]]\n",
    "theta_post= ['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_']\n",
    "X_theta_post= X_theta[[col for col in X_theta.columns if any(col.startswith(prefix) for prefix in theta_post)]]\n",
    "print('Theta Features:')\n",
    "print('Total: ' + str(len(X_theta.columns)))\n",
    "print('Pre: ' + str(len(X_theta_pre.columns)))\n",
    "print('Post: ' + str(len(X_theta_post.columns)))\n",
    "\n",
    "frequency_prefixes= ['1_','2_','3_','4_','5_','6_','7_','8_','9_','10_','11_','12_','13_','14_','15_','16_','17_','18_','19_','20_','21_','22_','23_','24_','25_','26_','27_','28_','29_','30_']\n",
    "X_frequency= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in frequency_prefixes)]]\n",
    "frequency_pre=['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_','25_','26_','27_','28_','29_','30_']\n",
    "X_frequency_pre= X_frequency[[col for col in X_frequency.columns if any(col.startswith(prefix) for prefix in frequency_pre)]]\n",
    "frequency_post=['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_','19_','20_','21_','22_','23_','24_']\n",
    "X_frequency_post= X_frequency[[col for col in X_frequency.columns if any(col.startswith(prefix) for prefix in frequency_post)]]\n",
    "print('Frequency Features:')\n",
    "print('Total: ' + str(len(X_frequency.columns)))\n",
    "print('Pre: ' + str(len(X_frequency_pre.columns)))\n",
    "print('Post: ' + str(len(X_frequency_post.columns)))\n",
    "\n",
    "temporal_prefixes= ['31_','32_','33_','34_','35_','36_']\n",
    "X_temporal= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in temporal_prefixes)]]\n",
    "temporal_pre= ['33_','34_']\n",
    "X_temporal_pre= X_temporal[[col for col in X_temporal.columns if any(col.startswith(prefix) for prefix in temporal_pre)]] \n",
    "temporal_post= ['31_','32_','35_','36_']\n",
    "X_temporal_post= X_temporal[[col for col in X_temporal.columns if any(col.startswith(prefix) for prefix in temporal_post)]]\n",
    "print('Temporal Features:')\n",
    "print('Total: ' + str(len(X_temporal.columns)))\n",
    "print('Pre: ' + str(len(X_temporal_pre.columns)))\n",
    "print('Post: ' + str(len(X_temporal_post.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=features[features['labels']==0]\n",
    "erroneous=features[features['labels']==1]\n",
    "correct_sample = correct.sample(n=2500, random_state=42)\n",
    "erroneous_sample = erroneous.sample(n=900, random_state=42)\n",
    "X_for_grid= pd.concat([correct_sample, erroneous_sample])\n",
    "Y_for_grid= X_for_grid[['labels']]\n",
    "best_params= src.grid_search(X_for_grid, Y_for_grid)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.functions' from 'c:\\\\Users\\\\User\\\\Documents\\\\GitHub\\\\Tese\\\\src\\\\functions.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_theta_features1.txt', 'r') as file:\n",
    "    best_theta_features = file.read().split(', ')\n",
    "\n",
    "X= X_theta[best_theta_features]\n",
    "src.classification(len(best_theta_features),X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_frequency_features1.txt', 'r') as file:\n",
    "    best_frequency_features = file.read().split(', ')\n",
    "\n",
    "X= X_frequency[best_frequency_features]\n",
    "src.classification(len(best_frequency_features),X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_temporal_features1.txt', 'r') as file:\n",
    "    best_temporal_features = file.read().split(', ')\n",
    "\n",
    "X= X_temporal[best_temporal_features]\n",
    "src.classification(len(best_temporal_features),X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_all_features1.txt', 'r') as file:\n",
    "    best_all_features = file.read().split(', ')\n",
    "\n",
    "X= X_all[best_all_features]\n",
    "print(best_all_features)\n",
    "params= {'kernel': 'rbf'}\n",
    "test_size=0.1\n",
    "bal_acc=src.classification(len(best_all_features),X,Y, params,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc #teste de proporções, testar quao acima do chance level está\n",
    "#ROC curve\n",
    "#R^2\n",
    "# Best parameters found: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: {} [1_data    0.042799\n",
      "27_P4    -8.542211\n",
      "13_POZ   -8.546696\n",
      "8_PO4    -8.546976\n",
      "12_PZ    -8.547600\n",
      "8_FPZ    -8.548405\n",
      "20_CZ    -8.548460\n",
      "19_CP1   -8.548762\n",
      "27_PO6   -8.549048\n",
      "11_FC6   -8.549109\n",
      "33_FC2   -8.549381\n",
      "28_P7    -8.549476\n",
      "14_P6    -8.549521\n",
      "14_FZ    -8.549817\n",
      "8_F5     -8.549995\n",
      "29_P5    -8.550093\n",
      "8_AF3    -8.550193\n",
      "21_PO4   -8.550544\n",
      "11_PO4   -8.550765\n",
      "33_P3    -8.550938\n",
      "dtype: float64]\n",
      "Test size: 0.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_params\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m----> 3\u001b[0m n_feature, feature_df,scores, clf, scaler\u001b[38;5;241m=\u001b[39m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m best_all_features\u001b[38;5;241m=\u001b[39mfeature_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\GitHub\\Tese\\src\\functions.py:762\u001b[0m, in \u001b[0;36mbest_n_features\u001b[1;34m(X, Y, params)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 762\u001b[0m     bal_accuracy, clf, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m     bal_acc_p_feature\u001b[38;5;241m.\u001b[39mappend(bal_accuracy)\n\u001b[0;32m    764\u001b[0m     clf1_list\u001b[38;5;241m.\u001b[39mappend(clf)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\GitHub\\Tese\\src\\functions.py:520\u001b[0m, in \u001b[0;36mclassification\u001b[1;34m(i, X, Y, params, test_size)\u001b[0m\n\u001b[0;32m    518\u001b[0m clf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[0;32m    519\u001b[0m Y_predicted \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m--> 520\u001b[0m Y_predicted_prob \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    522\u001b[0m f1score\u001b[38;5;241m.\u001b[39mappend(f1_score(Y_test, Y_predicted))\n\u001b[0;32m    523\u001b[0m precision\u001b[38;5;241m.\u001b[39mappend(precision_score(Y_test, Y_predicted))  \u001b[38;5;66;03m# Precision = Positive predictive value\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:865\u001b[0m, in \u001b[0;36mBaseSVC.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\n\u001b[0;32m    860\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba is not available when fitted with probability=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    861\u001b[0m     )\n\u001b[0;32m    862\u001b[0m pred_proba \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict_proba \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict_proba\n\u001b[0;32m    864\u001b[0m )\n\u001b[1;32m--> 865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpred_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_base.py:905\u001b[0m, in \u001b[0;36mBaseSVC._dense_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    902\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[1;32m--> 905\u001b[0m pprob \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pprob\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "best_params= {'kernel': 'rbf'}\n",
    "n_feature, feature_df,scores, clf, scaler=src.best_n_features(X_all,Y,best_params)\n",
    "best_all_features=feature_df.columns.tolist()\n",
    "print(scores)\n",
    "print(best_all_features)\n",
    "print(best_params)\n",
    "\n",
    "if os.path.exists('best_all_features2.txt'):\n",
    "    os.remove('best_all_features2.txt')\n",
    "with open('best_all_features2.txt', 'w') as file:\n",
    "    file.write(', '.join(best_all_features))\n",
    "print(\"File created and list stored successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.functions as src\n",
    "import os \n",
    "\n",
    "features = \"dataset1_features.csv\"\n",
    "features = pd.read_csv(features, sep=',', header=0, index_col=None)\n",
    "Y = features[['labels']]\n",
    "\n",
    "X_all1 = features.drop('labels', axis=1)\n",
    "all_pre= ['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_','25_','26_','27_','28_','29_','30_','33_','34_']\n",
    "X_all_pre=X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in all_pre)]]\n",
    "all_post=  ['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_','19_','20_','21_','22_','23_','24_','31_','32_','35_','36_']\n",
    "X_all_post=X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in all_post)]]\n",
    "print('All Features:')\n",
    "print('Total: ' + str(len(X_all.columns))) \n",
    "print('Pre: ' + str(len(X_all_pre.columns)))\n",
    "print('Post: ' + str(len(X_all_post.columns)))\n",
    "\n",
    "theta_prefixes= ['1_','2_','3_','4_','5_','6_','7_','8_','9_','10_','11_','12_','13_','14_','15_','16_','17_','18_']\n",
    "X_theta= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in theta_prefixes)]]\n",
    "theta_pre= ['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_']\n",
    "X_theta_pre= X_theta[[col for col in X_theta.columns if any(col.startswith(prefix) for prefix in theta_pre)]]\n",
    "theta_post= ['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_']\n",
    "X_theta_post= X_theta[[col for col in X_theta.columns if any(col.startswith(prefix) for prefix in theta_post)]]\n",
    "print('Theta Features:')\n",
    "print('Total: ' + str(len(X_theta.columns)))\n",
    "print('Pre: ' + str(len(X_theta_pre.columns)))\n",
    "print('Post: ' + str(len(X_theta_post.columns)))\n",
    "\n",
    "frequency_prefixes= ['1_','2_','3_','4_','5_','6_','7_','8_','9_','10_','11_','12_','13_','14_','15_','16_','17_','18_','19_','20_','21_','22_','23_','24_','25_','26_','27_','28_','29_','30_']\n",
    "X_frequency= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in frequency_prefixes)]]\n",
    "frequency_pre=['3_','4_','6_','7_','10_','11_','12_','15_','16_','18_','25_','26_','27_','28_','29_','30_']\n",
    "X_frequency_pre= X_frequency[[col for col in X_frequency.columns if any(col.startswith(prefix) for prefix in frequency_pre)]]\n",
    "frequency_post=['1_','2_','5_','7_','8_','9_','12_','13_','14_','17_','19_','20_','21_','22_','23_','24_']\n",
    "X_frequency_post= X_frequency[[col for col in X_frequency.columns if any(col.startswith(prefix) for prefix in frequency_post)]]\n",
    "print('Frequency Features:')\n",
    "print('Total: ' + str(len(X_frequency.columns)))\n",
    "print('Pre: ' + str(len(X_frequency_pre.columns)))\n",
    "print('Post: ' + str(len(X_frequency_post.columns)))\n",
    "\n",
    "temporal_prefixes= ['31_','32_','33_','34_','35_','36_']\n",
    "X_temporal= X_all[[col for col in X_all.columns if any(col.startswith(prefix) for prefix in temporal_prefixes)]]\n",
    "temporal_pre= ['33_','34_']\n",
    "X_temporal_pre= X_temporal[[col for col in X_temporal.columns if any(col.startswith(prefix) for prefix in temporal_pre)]] \n",
    "temporal_post= ['31_','32_','35_','36_']\n",
    "X_temporal_post= X_temporal[[col for col in X_temporal.columns if any(col.startswith(prefix) for prefix in temporal_post)]]\n",
    "print('Temporal Features:')\n",
    "print('Total: ' + str(len(X_temporal.columns)))\n",
    "print('Pre: ' + str(len(X_temporal_pre.columns)))\n",
    "print('Post: ' + str(len(X_temporal_post.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def test_on_second_dataset(X_test_2, Y_test_2, trained_clf, trained_scaler):\n",
    "    # Standardize the second dataset using the scaler from the training step\n",
    "    X_test_2_scaled = trained_scaler.transform(X_test_2)\n",
    "    \n",
    "    # Predict the labels for the second dataset\n",
    "    Y_pred_2 = trained_clf.predict(X_test_2_scaled)\n",
    "    \n",
    "    # Predict the probabilities for the second dataset (optional, if you need probabilities)\n",
    "    Y_pred_prob_2 = trained_clf.predict_proba(X_test_2_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate the balanced accuracy\n",
    "    bal_acc_2 = balanced_accuracy_score(Y_test_2, Y_pred_2)\n",
    "    \n",
    "    # Calculate other metrics as needed\n",
    "    f1 = f1_score(Y_test_2, Y_pred_2)\n",
    "    precision = precision_score(Y_test_2, Y_pred_2)\n",
    "    recall = recall_score(Y_test_2, Y_pred_2)\n",
    "    roc_auc = roc_auc_score(Y_test_2, Y_pred_prob_2)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Balanced Accuracy on second dataset: {:.4f}\".format(bal_acc_2))\n",
    "    print(\"F1 Score: {:.4f}\".format(f1))\n",
    "    print(\"Precision: {:.4f}\".format(precision))\n",
    "    print(\"Recall: {:.4f}\".format(recall))\n",
    "    print(\"ROC AUC Score: {:.4f}\".format(roc_auc))\n",
    "    \n",
    "    return Y_pred_2, Y_pred_prob_2, bal_acc_2\n",
    "\n",
    "\n",
    "# Assuming trained_clf and trained_scaler are your trained classifier and scaler respectively\n",
    "Y_pred_2, Y_pred_prob_2, bal_acc_2 = test_on_second_dataset(X_all1[best_all_features], Y, clf, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
