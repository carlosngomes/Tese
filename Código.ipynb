{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import src.functions as src\n",
    "\n",
    "\n",
    "files = os.listdir('data')\n",
    "file = [item for item in files if item.endswith('set')]\n",
    "fs=500\n",
    "freq_bands={'delta' : [0,4],\n",
    "            'theta' : [4,8],\n",
    "            'alpha' : [8, 13],\n",
    "            'beta' : [13,35],\n",
    "            'high' : [35, 45],\n",
    "            'all': [1.5,45],\n",
    "            'low': [1.5,12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = {}\n",
    "\n",
    "for file_index, file_name in enumerate(file):\n",
    "    path_ = os.path.join( 'data', file_name )\n",
    "    dados = mne.read_epochs_eeglab(path_)\n",
    "    channel_names = dados.info['ch_names']\n",
    "    subject_id = file_name[:3]\n",
    "    \n",
    "    # Extract labels\n",
    "    proCorr_antiCorr_labels = len(dados['proCorr','antiCorr'].get_data())\n",
    "    proErr_antiErr_nogoErr_labels = len(dados['proErr','antiErr','nogoErr'].get_data())\n",
    "    labels[subject_id] = np.array([0] * proCorr_antiCorr_labels + [1] * proErr_antiErr_nogoErr_labels)\n",
    "    \n",
    "    # Initialize channel data dictionary\n",
    "    channel_data = {}\n",
    "    \n",
    "    # Iterate over 'proCorr' and 'antiCorr' events\n",
    "    for epoch_values in dados['proCorr','antiCorr'].get_data():\n",
    "        for channel_index, channel_values in enumerate(epoch_values):\n",
    "            channel_name = channel_names[channel_index]\n",
    "            if channel_name not in channel_data:\n",
    "                channel_data[channel_name] = []\n",
    "            channel_data[channel_name].append(list(channel_values))\n",
    "    \n",
    "    # Iterate over 'proErr', 'antiErr', and 'nogoErr' events\n",
    "    for epoch_values in dados['proErr','antiErr','nogoErr'].get_data():\n",
    "        for channel_index, channel_values in enumerate(epoch_values):\n",
    "            channel_name = channel_names[channel_index]\n",
    "            if channel_name not in channel_data:\n",
    "                channel_data[channel_name] = []\n",
    "            channel_data[channel_name].append(list(channel_values))\n",
    "    \n",
    "    data[subject_id] = channel_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting into pre- and post-response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_response={}\n",
    "post_response={}\n",
    "for subject_id, dados in data.items():\n",
    "    channel_data_pre={}\n",
    "    channel_data_post={}\n",
    "    for channel_name, values in dados.items():\n",
    "        values_pre=[]\n",
    "        values_post=[]\n",
    "        for i in range(len(values)):\n",
    "            \n",
    "            index= int(len(values[i])/2)\n",
    "            values_pre.append(values[i][0:index])\n",
    "            values_post.append(values[i][index:-1])\n",
    "        channel_data_pre[channel_name]= values_pre\n",
    "        channel_data_post[channel_name]= values_post\n",
    "         \n",
    "    pre_response[subject_id] = channel_data_pre #(14,60,n_events,250)\n",
    "    post_response[subject_id] = channel_data_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pre,S_pre= src.getpsd(pre_response,fs)    # S_pre ->(14,60,n_events,n_points)\n",
    "f_post,S_post= src.getpsd(post_response,fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting PSD data per subject per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_per_channel= src.getdataperchannel(S_pre, channel_names) #(14,60,n_events,n_points)\n",
    "post_data_per_channel= src.getdataperchannel(S_post,channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means = {}\n",
    "\n",
    "for channel_name, values in post_data_per_channel['P01'].items():\n",
    "    mean_array = []\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        mean_array.append(values[i])\n",
    "    \n",
    "    channel_means[channel_name] = src.mean_of_lists(mean_array)\n",
    "\n",
    "for channel_name, means in channel_means.items():\n",
    "    plt.plot(f_post['P01'], np.log(means))\n",
    "\n",
    "# Customize plot\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [log(V**2/Hz)]')\n",
    "plt.title('Mean Event PSD per channel')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means = {}\n",
    "\n",
    "for channel_name, values in post_data_per_channel['P02'].items():\n",
    "    mean_array = []\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        mean_array.append(values[i])\n",
    "    channel_means[channel_name] = src.mean_of_lists(mean_array)\n",
    "\n",
    "for channel_name, means in channel_means.items():\n",
    "    plt.plot(f_post['P02'], np.log(means))\n",
    "\n",
    "# Customize plot\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [log(V**2/Hz)]')\n",
    "plt.title('Mean Event PSD per channel')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCZ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1= src.feature('fcz_features','theta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post, f_post)\n",
    "feature2= src.feature('fcz_features','theta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature3= src.feature('fcz_features','theta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature4=src.feature('fcz_features','theta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature5=src.feature('fcz_features','theta','other',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature6=src.feature('fcz_features','theta','other',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature7= src.feature('fcz_features','theta','theta',post_data_per_channel,pre_data_per_channel,freq_bands,f_post,f_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Channels Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature8= src.feature('all_features','theta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature9= src.feature('all_features','theta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature10= src.feature('all_features','theta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature11= src.feature('all_features','theta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature12= src.feature('all_features','theta','theta',post_data_per_channel,pre_data_per_channel,freq_bands,f_post,f_pre)\n",
    "feature13= src.feature('all_features','theta','delta',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature14= src.feature('all_features','theta','alpha',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature15= src.feature('all_features','theta','delta',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature16= src.feature('all_features','theta','alpha',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midfrontal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature17= src.feature('midfrontal_features','theta','other',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature18= src.feature('midfrontal_features','theta','other',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta, Alpha and Low Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature19= src.feature('low_features','delta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature20= src.feature('low_features','alpha','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature21= src.feature('low_features','low','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature22= src.feature('low_features','delta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature23= src.feature('low_features','alpha','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature24= src.feature('low_features','low','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post)\n",
    "feature25= src.feature('low_features','delta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature26= src.feature('low_features','alpha','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature27= src.feature('low_features','low','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature28= src.feature('low_features','delta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature29= src.feature('low_features','alpha','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)\n",
    "feature30= src.feature('low_features','low','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature31={} \n",
    "feature32={}\n",
    "feature33={}\n",
    "feature34={}\n",
    "feature35={}\n",
    "feature36={}\n",
    "Cluster_Pz=['CPZ','P1','PZ','P2','POZ']\n",
    "Cluster_FCZ=['FZ','FC1','FCZ','FC2','CZ']\n",
    "for subject_id,values in post_response.items():\n",
    "    mean_ERN_values = []\n",
    "    mean_Pe_values = []\n",
    "    mean_all0_250_values = []\n",
    "    mean_all250_500_values = []\n",
    "\n",
    "    # Create a range object that will iterate over the indices of events \n",
    "    for i in range(len(next(iter(values.values())))):\n",
    "\n",
    "        ERN_values = []\n",
    "        Pe_values = []\n",
    "        all0_250_values = []\n",
    "        all250_500_values = []\n",
    "        \n",
    "        for channel_name, channel_values in values.items():\n",
    "            if channel_name in Cluster_FCZ:\n",
    "                ERN_values.append(np.mean(channel_values[i][35:81]))  # Potential's list for [70,160]ms for each event\n",
    "            elif channel_name in Cluster_Pz:\n",
    "                Pe_values.append(np.mean(channel_values[i][100:-1]))\n",
    "            \n",
    "            all0_250_values.append(np.mean(channel_values[i][0:126]))\n",
    "            all250_500_values.append(np.mean(channel_values[i][125:-1]))\n",
    "        \n",
    "        mean_ERN_values.append(np.mean(ERN_values))\n",
    "        mean_Pe_values.append(np.mean(Pe_values))\n",
    "        mean_all0_250_values.append(np.mean(all0_250_values))\n",
    "        mean_all250_500_values.append(np.mean(all250_500_values))\n",
    "    feature31[subject_id]= mean_ERN_values\n",
    "    feature32[subject_id]=mean_Pe_values\n",
    "    feature35[subject_id]=mean_all0_250_values\n",
    "    feature36[subject_id]=mean_all250_500_values\n",
    "\n",
    "\n",
    "for subject_id,values in pre_response.items():\n",
    "    mean_all500_250_values = []\n",
    "    mean_all250_0_values = []\n",
    "    for i in range(len(next(iter(values.values())))):\n",
    "        \n",
    "        all500_250_values = []\n",
    "        all250_0_values = []\n",
    "        \n",
    "        for channel_name, channel_values in values.items():            \n",
    "            all500_250_values.append(np.mean(channel_values[i][0:126]))\n",
    "            all250_0_values.append(np.mean(channel_values[i][125:-1]))\n",
    "        \n",
    "        mean_all500_250_values.append(np.mean(all500_250_values))\n",
    "        mean_all250_0_values.append(np.mean(all250_0_values))\n",
    "    feature33[subject_id]= mean_all500_250_values\n",
    "    feature34[subject_id]=mean_all250_0_values#(14,n_events)\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_features = {}\n",
    "for feature_index in range(1, 31):\n",
    "    feature_corr_key = 'feature{}'.format(feature_index)\n",
    "    \n",
    "    # Iterate over subjects and their respective values for the current feature\n",
    "    for subject_id, values_list in globals()[feature_corr_key].items():\n",
    "        # If subject_id not in all_subjects_data, initialize it with an empty list\n",
    "        if subject_id not in frequency_features:\n",
    "            frequency_features[subject_id] = []\n",
    "\n",
    "        # Ensure that all_subjects_data[subject_id] has enough lists to store event values\n",
    "        while len(frequency_features[subject_id]) < len(values_list):\n",
    "            frequency_features[subject_id].append([])\n",
    "\n",
    "        # Accumulate values for each event\n",
    "        for event_index, value in enumerate(values_list):\n",
    "            frequency_features[subject_id][event_index].append(value) #(14,n_events,n_features)\n",
    "\n",
    "\n",
    "theta_features={}\n",
    "for feature_index in range(1, 19):\n",
    "    feature_corr_key = 'feature{}'.format(feature_index)\n",
    "    \n",
    "    for subject_id, values_list in globals()[feature_corr_key].items():\n",
    "\n",
    "        if subject_id not in theta_features:\n",
    "            theta_features[subject_id] = []\n",
    "\n",
    "        while len(theta_features[subject_id]) < len(values_list):\n",
    "            theta_features[subject_id].append([])\n",
    "\n",
    "        for event_index, value in enumerate(values_list):\n",
    "            theta_features[subject_id][event_index].append(value)\n",
    "\n",
    "\n",
    "temporal_features={}\n",
    "for feature_index in range(31, 37):\n",
    "    feature_corr_key = 'feature{}'.format(feature_index)\n",
    "    \n",
    "    for subject_id, values_list in globals()[feature_corr_key].items():\n",
    "\n",
    "        if subject_id not in temporal_features:\n",
    "            temporal_features[subject_id] = []\n",
    "\n",
    "        while len(temporal_features[subject_id]) < len(values_list):\n",
    "            temporal_features[subject_id].append([])\n",
    "\n",
    "        for event_index, value in enumerate(values_list):\n",
    "            temporal_features[subject_id][event_index].append(value)\n",
    "\n",
    "all_features={}\n",
    "for feature_index in range(1, 37):\n",
    "    feature_corr_key = 'feature{}'.format(feature_index)\n",
    "    \n",
    "    for subject_id, values_list in globals()[feature_corr_key].items():\n",
    "\n",
    "        if subject_id not in all_features:\n",
    "            all_features[subject_id] = []\n",
    "\n",
    "        while len(all_features[subject_id]) < len(values_list):\n",
    "            all_features[subject_id].append([])\n",
    "\n",
    "        for event_index, value in enumerate(values_list):\n",
    "            all_features[subject_id][event_index].append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.save_dict_to_file(all_features, 'all_features.txt')\n",
    "src.save_dict_to_file(frequency_features, 'frequency_features.txt')\n",
    "src.save_dict_to_file(theta_features, 'theta_features.txt')\n",
    "src.save_dict_to_file(temporal_features, 'temporal_features.txt')\n",
    "src.save_dict_to_file(labels,'labels.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
