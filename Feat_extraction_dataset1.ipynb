{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import src.functions as src\n",
    "\n",
    "\n",
    "fs=500\n",
    "freq_bands={'delta' : [1.5,3.5],\n",
    "            'theta' : [4,8],\n",
    "            'alpha' : [8.5, 12],\n",
    "            'beta' : [13,35],\n",
    "            'high' : [35, 45],\n",
    "            'all': [1.5,45],\n",
    "            'low': [1.5,12]}\n",
    "\n",
    "\n",
    "data = {}\n",
    "labels = {}\n",
    "file= [item for item in os.listdir('data') if item.endswith('set')]\n",
    "\n",
    "for file_index, file_name in enumerate(file):\n",
    "    path_ = os.path.join( 'data', file_name )\n",
    "    dados = mne.read_epochs_eeglab(path_)\n",
    "    channel_names = dados.info['ch_names']\n",
    "    subject_id = file_name[:3]\n",
    "    \n",
    "    # Extract labels\n",
    "    Corr_labels = len(dados['proCorr','antiCorr'].get_data())\n",
    "    Err_labels = len(dados['proErr','antiErr','nogoErr'].get_data())\n",
    "    labels[subject_id] = np.array([0] * Corr_labels + [1] * Err_labels)\n",
    "    \n",
    "    # Initialize channel data dictionary\n",
    "    channel_data = {}\n",
    "    \n",
    "    # Iterate over 'proCorr' and 'antiCorr' events\n",
    "    for epoch_values in dados['proCorr','antiCorr'].get_data():\n",
    "        for channel_index, channel_values in enumerate(epoch_values):\n",
    "            channel_name = channel_names[channel_index]\n",
    "            if channel_name not in channel_data:\n",
    "                channel_data[channel_name] = []\n",
    "            channel_data[channel_name].append(list(channel_values))\n",
    "    \n",
    "    # Iterate over 'proErr', 'antiErr', and 'nogoErr' events\n",
    "    for epoch_values in dados['proErr','antiErr','nogoErr'].get_data():\n",
    "        for channel_index, channel_values in enumerate(epoch_values):\n",
    "            channel_name = channel_names[channel_index]\n",
    "            if channel_name not in channel_data:\n",
    "                channel_data[channel_name] = []\n",
    "            channel_data[channel_name].append(list(channel_values))\n",
    "    \n",
    "    data[subject_id] = channel_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_post(data):\n",
    "    pre_response={}\n",
    "    post_response={}\n",
    "    for subject_id, dados in data.items():\n",
    "        channel_data_pre={}\n",
    "        channel_data_post={}\n",
    "        for channel_name, values in dados.items():\n",
    "            values_pre=[]\n",
    "            values_post=[]\n",
    "            for i in range(len(values)):\n",
    "                \n",
    "                index= int(len(values[i])/2)\n",
    "                values_pre.append(values[i][0:index])\n",
    "                values_post.append(values[i][index:-1])\n",
    "            channel_data_pre[channel_name]= values_pre\n",
    "            channel_data_post[channel_name]= values_post\n",
    "            \n",
    "        pre_response[subject_id] = channel_data_pre #(14,60,n_events,250)\n",
    "        post_response[subject_id] = channel_data_post\n",
    "    return pre_response, post_response\n",
    "\n",
    "pre_response, post_response = data_pre_post(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pre,S_pre= src.getpsd(pre_response,fs)    # S_pre ->(14,60,n_events,n_points)\n",
    "f_post,S_post= src.getpsd(post_response,fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_per_channel= src.getdataperchannel(S_pre) #(subject_id,60,n_events,n_points)\n",
    "post_data_per_channel= src.getdataperchannel(S_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCZ cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= src.extend_list(src.feature('fcz_features','.','.','theta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post, f_post))\n",
    "data2= src.extend_list(src.feature('fcz_features','.','.','theta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "data3= src.extend_list(src.feature('fcz_features','.','.','theta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "data4= src.extend_list(src.feature('fcz_features','.','.','theta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "data5= src.extend_list(src.feature('fcz_features','.','.','theta','other',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "data6= src.extend_list(src.feature('fcz_features','.','.','theta','other',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "data7= src.extend_list(src.feature('fcz_features','.','.','theta','theta',post_data_per_channel,pre_data_per_channel,freq_bands,f_post,f_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8={}\n",
    "data9={}\n",
    "data10={}\n",
    "data11={}\n",
    "data12={}\n",
    "data13={}\n",
    "data14={}\n",
    "data15={}\n",
    "data16={}\n",
    "for channel_name in post_data_per_channel['P01'].keys():    \n",
    "    feature8= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature9= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature10= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature11= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature12= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','theta',post_data_per_channel,pre_data_per_channel,freq_bands,f_post,f_pre))\n",
    "    feature13= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','delta',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature14= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','alpha',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature15= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','delta',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature16= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','alpha',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    data8[channel_name]=feature8\n",
    "    data9[channel_name]=feature9\n",
    "    data10[channel_name]=feature10\n",
    "    data11[channel_name]=feature11\n",
    "    data12[channel_name]=feature12\n",
    "    data13[channel_name]=feature13\n",
    "    data14[channel_name]=feature14\n",
    "    data15[channel_name]=feature15\n",
    "    data16[channel_name]=feature16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midfrontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data17={}\n",
    "data18={}\n",
    "for channel_name in post_data_per_channel['P01'].keys():\n",
    "    feature17= src.extend_list(src.feature('midfrontal_features',channel_name,channel_name,'theta','other',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature18= src.extend_list(src.feature('midfrontal_features',channel_name,channel_name,'theta','other',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    data17[channel_name]=feature17\n",
    "    data18[channel_name]=feature18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data19={}\n",
    "data20={}\n",
    "data21={}\n",
    "data22={}\n",
    "data23={}\n",
    "data24={}\n",
    "data25={}\n",
    "data26={}\n",
    "data27={}\n",
    "data28={}\n",
    "data29={}\n",
    "data30={}\n",
    "for channel_name in post_data_per_channel[subject_id].keys():        \n",
    "    \n",
    "    feature19= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature20= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature21= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','all',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature22= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature23= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature24= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','high',post_data_per_channel,post_data_per_channel,freq_bands,f_post,f_post))\n",
    "    feature25= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature26= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature27= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','all',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature28= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature29= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    feature30= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','high',pre_data_per_channel,pre_data_per_channel,freq_bands,f_pre,f_pre))\n",
    "    data19[channel_name]=feature19\n",
    "    data20[channel_name]=feature20\n",
    "    data21[channel_name]=feature21\n",
    "    data22[channel_name]=feature22\n",
    "    data23[channel_name]=feature23\n",
    "    data24[channel_name]=feature24\n",
    "    data25[channel_name]=feature25\n",
    "    data26[channel_name]=feature26\n",
    "    data27[channel_name]=feature27\n",
    "    data28[channel_name]=feature28\n",
    "    data29[channel_name]=feature29\n",
    "    data30[channel_name]=feature30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data31={}\n",
    "data32={}\n",
    "FCZ_Cluster=['FZ','FC1','FCZ','FC2','CZ']\n",
    "PE_Cluster=['CPZ','P1','PZ','P2','POZ']\n",
    "for subject_id,values in post_response.items():\n",
    "    mean_all70_160_values = []\n",
    "    mean_all200_500_values = []\n",
    "    for i in range(len(next(iter(values.values())))):\n",
    "        all70_160_values = []\n",
    "        all200_500_values = []\n",
    "        for channel_name, channel_values in values.items():   \n",
    "            if channel_name in FCZ_Cluster:         \n",
    "                all70_160_values.append(np.mean(channel_values[i][35:81]))\n",
    "            elif channel_name in PE_Cluster:\n",
    "                all200_500_values.append(np.mean(channel_values[i][100:-1]))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        mean_all70_160_values.append(np.mean(all70_160_values))\n",
    "        mean_all200_500_values.append(np.mean(all200_500_values))\n",
    "    data31[subject_id]= mean_all70_160_values\n",
    "    data32[subject_id]= mean_all200_500_values\n",
    "\n",
    "data31=src.extend_list(data31)\n",
    "data32=src.extend_list(data32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data33 = {}\n",
    "data34 = {}\n",
    "# Iterate over each subject in the pre_response2 dictionary\n",
    "for subject_id, values in pre_response.items():\n",
    "    # Iterate over each channel in the subject's data\n",
    "    for channel_name, channel_values in values.items():\n",
    "        # If the channel is not yet in the feature dictionaries, initialize it\n",
    "        if channel_name not in data33:\n",
    "            data33[channel_name] = []\n",
    "        if channel_name not in data34:\n",
    "            data34[channel_name] = []\n",
    "        \n",
    "        # Iterate over each event in the channel values\n",
    "        for i in range(len(channel_values)):\n",
    "            # Calculate the mean values for the specified ranges\n",
    "            mean_500_250 = np.mean(channel_values[i][0:126])\n",
    "            mean_250_0 = np.mean(channel_values[i][125:-1])\n",
    "            \n",
    "            # Append the mean values to the corresponding channel's list in the feature dictionaries\n",
    "            data33[channel_name].append(mean_500_250)\n",
    "            data34[channel_name].append(mean_250_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data35 = {}\n",
    "data36 = {}\n",
    "# Iterate over each subject in the pre_response2 dictionary\n",
    "for subject_id, values in post_response.items():\n",
    "    # Iterate over each channel in the subject's data\n",
    "    for channel_name, channel_values in values.items():\n",
    "        # If the channel is not yet in the feature dictionaries, initialize it\n",
    "        if channel_name not in data35:\n",
    "            data35[channel_name] = []\n",
    "        if channel_name not in data36:\n",
    "            data36[channel_name] = []\n",
    "        \n",
    "        # Iterate over each event in the channel values\n",
    "        for i in range(len(channel_values)):\n",
    "            # Calculate the mean values for the specified ranges\n",
    "            mean_0_250 = np.mean(channel_values[i][0:126])\n",
    "            mean_250_500 = np.mean(channel_values[i][125:-1])\n",
    "            \n",
    "            # Append the mean values to the corresponding channel's list in the feature dictionaries\n",
    "            data35[channel_name].append(mean_0_250)\n",
    "            data36[channel_name].append(mean_250_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'dataset1_features.csv'\n",
    "\n",
    "# Check if the file exists and remove it\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Create an empty list to store the dictionaries\n",
    "dicts = []\n",
    "\n",
    "# Iterate over the range to collect data dictionaries\n",
    "for feature_index in range(1, 37):\n",
    "    feature_corr_key = 'data{}'.format(feature_index)\n",
    "    dicts.append(globals().get(feature_corr_key, {}))\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each dictionary/list and its index\n",
    "for index, feature_data in enumerate(dicts, start=1):\n",
    "    # Check if feature_data is a dictionary\n",
    "    if isinstance(feature_data, dict):\n",
    "        # Filter out channels with missing or empty data\n",
    "        filtered_dict = {channel: data for channel, data in feature_data.items() if data}\n",
    "\n",
    "        # Convert the filtered dictionary to a DataFrame\n",
    "        if filtered_dict:\n",
    "            try:\n",
    "                print(f\"Processing dictionary at index {index}\")\n",
    "                df = pd.DataFrame(filtered_dict)\n",
    "                # Rename the columns to include the iteration number\n",
    "                df.columns = [f'{index}_{col}' for col in df.columns]\n",
    "                # Concatenate the DataFrame to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], axis=1)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred for dictionary at index {index}: {e}\")\n",
    "                continue\n",
    "    # Check if feature_data is a list\n",
    "    elif isinstance(feature_data, list):\n",
    "        if feature_data:\n",
    "            try:\n",
    "                print(f\"Processing list at index {index}\")\n",
    "                df = pd.DataFrame({f'{index}_data': feature_data})\n",
    "                # Concatenate the DataFrame to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], axis=1)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred for list at index {index}: {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        print(f\"Skipping index {index} as it is neither a dictionary nor a list.\")\n",
    "\n",
    "# Extend the labels list and add it as a new column in the combined DataFrame\n",
    "combined_df['labels'] = src.extend_list(labels)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
