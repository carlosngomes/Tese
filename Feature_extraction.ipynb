{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import src.functions as src\n",
    "\n",
    "fs=500\n",
    "freq_bands={'delta' : [0,4],\n",
    "            'theta' : [4,8],\n",
    "            'alpha' : [8, 13],\n",
    "            'beta' : [13,35],\n",
    "            'high' : [35, 45],\n",
    "            'all': [1.5,45],\n",
    "            'low': [1.5,12]}\n",
    "\n",
    "data2={}\n",
    "labels2={}\n",
    "path='Dataset2'\n",
    "for files in os.listdir(path):\n",
    "    folders=os.path.join(path,files)\n",
    "    file2=[item for item in os.listdir(folders) if item.endswith('set')]\n",
    "#--------------------------------------------------------------------------\n",
    "    i=0\n",
    "    label=[]\n",
    "    while i!=4:\n",
    "        for file_index, file_name in enumerate(file2):\n",
    "            i=i+1\n",
    "            path_ = os.path.join( folders, file_name )\n",
    "            dados = mne.read_epochs_eeglab(path_)\n",
    "            channel_names2 = dados.info['ch_names']\n",
    "            subject_id = file_name[:3]\n",
    "            \n",
    "            # Extract labels\n",
    "            try:\n",
    "                Corr_labels = int(len(dados['150','160'].get_data()))\n",
    "            except:\n",
    "                Corr_labels=0\n",
    "            try:\n",
    "                Err_labels = int(len(dados['151','152','153','154','161','162','163','164'].get_data()))\n",
    "            except:\n",
    "                Err_labels=0\n",
    "            label.append(np.array([0] * Corr_labels + [1] * Err_labels))\n",
    "           \n",
    "    #--------------------------------------------------------------------------    \n",
    "            # Initialize channel data dictionary\n",
    "            channel_data = {}\n",
    "            data_channel=[]\n",
    "            try:\n",
    "                for epoch_values in dados['150','160'].get_data():\n",
    "                    for channel_index, channel_values in enumerate(epoch_values):\n",
    "                        channel_name = channel_names2[channel_index]\n",
    "                        if channel_name not in channel_data:\n",
    "                            channel_data[channel_name] = []\n",
    "                        channel_data[channel_name].append(list(channel_values))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                for epoch_values in dados['151','152','153','154','161','162','163','164'].get_data():\n",
    "                    for channel_index, channel_values in enumerate(epoch_values):\n",
    "                        channel_name = channel_names2[channel_index]\n",
    "                        if channel_name not in channel_data:\n",
    "                            channel_data[channel_name] = []\n",
    "                        channel_data[channel_name].append(list(channel_values))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            data2[file_name] = channel_data\n",
    "        labels2[subject_id] = np.concatenate(label)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "rearranged_data = {}\n",
    "\n",
    "# Iterate through each file in data2\n",
    "for file_name, channel_data in data2.items():\n",
    "    # Extract the subject ID from the file name\n",
    "    subject_id = file_name.split('_')[0]\n",
    "    \n",
    "    # Initialize the subject in the rearranged_data dictionary if not already present\n",
    "    if subject_id not in rearranged_data:\n",
    "        rearranged_data[subject_id] = {}\n",
    "    \n",
    "    # Iterate through each channel in the current file\n",
    "    for channel, data in channel_data.items():\n",
    "        # Initialize the channel in the subject's dictionary if not already present\n",
    "        if channel not in rearranged_data[subject_id]:\n",
    "            rearranged_data[subject_id][channel] = []\n",
    "        \n",
    "        # Append the current file's data for the channel to the subject's channel data\n",
    "        rearranged_data[subject_id][channel].append(data)\n",
    "\n",
    "# Combine the data from different files of the same subject for each channel\n",
    "for subject_id, channels in rearranged_data.items():\n",
    "    for channel, data_list in channels.items():\n",
    "        # Flatten the list of data arrays into a single list or concatenate them\n",
    "        # Here we assume each data is a list or array; if not, adjust accordingly\n",
    "        combined_data = []\n",
    "        for data in data_list:\n",
    "            combined_data.extend(data)\n",
    "        rearranged_data[subject_id][channel] = combined_data\n",
    "\n",
    "# Now rearranged_data is in the desired format\n",
    "# You can print or further process the rearranged_data as needed\n",
    "\n",
    "data2=rearranged_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_post(data):\n",
    "    pre_response={}\n",
    "    post_response={}\n",
    "    for subject_id, dados in data.items():\n",
    "        channel_data_pre={}\n",
    "        channel_data_post={}\n",
    "        for channel_name, values in dados.items():\n",
    "            values_pre=[]\n",
    "            values_post=[]\n",
    "            for i in range(len(values)):\n",
    "                \n",
    "                index= int(len(values[i])/2)\n",
    "                values_pre.append(values[i][0:index])\n",
    "                values_post.append(values[i][index:-1])\n",
    "            channel_data_pre[channel_name]= values_pre\n",
    "            channel_data_post[channel_name]= values_post\n",
    "            \n",
    "        pre_response[subject_id] = channel_data_pre #(14,60,n_events,250)\n",
    "        post_response[subject_id] = channel_data_post\n",
    "    return pre_response, post_response\n",
    "\n",
    "pre_response2, post_response2 = data_pre_post(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pre2,S_pre2= src.getpsd(pre_response2,fs)    # S_pre ->(14,60,n_events,n_points)\n",
    "f_post2,S_post2= src.getpsd(post_response2,fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data_per_channel2= src.getdataperchannel(S_pre2) #(subject_id,60,n_events,n_points)\n",
    "post_data_per_channel2= src.getdataperchannel(S_post2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1= src.feature('fcz_features','.','.','theta','all',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2, f_post2)\n",
    "feature2= src.feature('fcz_features','.','.','theta','high',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2)\n",
    "feature3= src.feature('fcz_features','.','.','theta','all',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2)\n",
    "feature4= src.feature('fcz_features','.','.','theta','high',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2)\n",
    "feature5= src.feature('fcz_features','.','.','theta','other',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2)\n",
    "feature6= src.feature('fcz_features','.','.','theta','other',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2)\n",
    "feature7= src.feature('fcz_features','.','.','theta','theta',post_data_per_channel2,pre_data_per_channel2,freq_bands,f_post2,f_pre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= feature1\n",
    "data2= feature2\n",
    "data3= feature3\n",
    "data4= feature4\n",
    "data5= feature5\n",
    "data6= feature6\n",
    "data7= feature7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8={}\n",
    "data9={}\n",
    "data10={}\n",
    "data11={}\n",
    "data12={}\n",
    "data13={}\n",
    "data14={}\n",
    "data15={}\n",
    "data16={}\n",
    "for channel_name in post_data_per_channel2['P01'].keys():    \n",
    "    feature8= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','all',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature9= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','high',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature10= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','all',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature11= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','high',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature12= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','theta',post_data_per_channel2,pre_data_per_channel2,freq_bands,f_post2,f_pre2))\n",
    "    feature13= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','delta',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature14= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','alpha',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature15= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','delta',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature16= src.extend_list(src.feature('all_features',channel_name,channel_name,'theta','alpha',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    data8[channel_name]=feature8\n",
    "    data9[channel_name]=feature9\n",
    "    data10[channel_name]=feature10\n",
    "    data11[channel_name]=feature11\n",
    "    data12[channel_name]=feature12\n",
    "    data13[channel_name]=feature13\n",
    "    data14[channel_name]=feature14\n",
    "    data15[channel_name]=feature15\n",
    "    data16[channel_name]=feature16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data17={}\n",
    "data18={}\n",
    "for channel_name in post_data_per_channel2['P01'].keys():\n",
    "    feature17= src.extend_list(src.feature('midfrontal_features',channel_name,channel_name,'theta','other',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature18= src.extend_list(src.feature('midfrontal_features',channel_name,channel_name,'theta','other',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    data17[channel_name]=feature17\n",
    "    data18[channel_name]=feature18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data19={}\n",
    "data20={}\n",
    "data21={}\n",
    "data22={}\n",
    "data23={}\n",
    "data24={}\n",
    "data25={}\n",
    "data26={}\n",
    "data27={}\n",
    "data28={}\n",
    "data29={}\n",
    "data30={}\n",
    "for channel_name in post_data_per_channel2[subject_id].keys():        \n",
    "    \n",
    "    feature19= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','all',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature20= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','all',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature21= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','all',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature22= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','high',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature23= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','high',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature24= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','high',post_data_per_channel2,post_data_per_channel2,freq_bands,f_post2,f_post2))\n",
    "    feature25= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','all',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature26= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','all',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature27= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','all',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature28= src.extend_list(src.feature('low_features',channel_name,channel_name,'delta','high',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature29= src.extend_list(src.feature('low_features',channel_name,channel_name,'alpha','high',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    feature30= src.extend_list(src.feature('low_features',channel_name,channel_name,'low','high',pre_data_per_channel2,pre_data_per_channel2,freq_bands,f_pre2,f_pre2))\n",
    "    data19[channel_name]=feature19\n",
    "    data20[channel_name]=feature20\n",
    "    data21[channel_name]=feature21\n",
    "    data22[channel_name]=feature22\n",
    "    data23[channel_name]=feature23\n",
    "    data24[channel_name]=feature24\n",
    "    data25[channel_name]=feature25\n",
    "    data26[channel_name]=feature26\n",
    "    data27[channel_name]=feature27\n",
    "    data28[channel_name]=feature28\n",
    "    data29[channel_name]=feature29\n",
    "    data30[channel_name]=feature30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature31={}\n",
    "feature32={}\n",
    "FCZ_Cluster=['FZ','FC1','FCZ','FC2','CZ']\n",
    "PE_Cluster=['CPZ','P1','PZ','P2','POZ']\n",
    "for subject_id,values in post_response2.items():\n",
    "    mean_all70_160_values = []\n",
    "    mean_all200_500_values = []\n",
    "    for i in range(len(next(iter(values.values())))):\n",
    "        all70_160_values = []\n",
    "        all200_500_values = []\n",
    "        for channel_name, channel_values in values.items():   \n",
    "            if channel_name in FCZ_Cluster:         \n",
    "                all70_160_values.append(np.mean(channel_values[i][35:81]))\n",
    "            elif channel_name in PE_Cluster:\n",
    "                all200_500_values.append(np.mean(channel_values[i][100:-1]))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        mean_all70_160_values.append(np.mean(all70_160_values))\n",
    "        mean_all200_500_values.append(np.mean(all200_500_values))\n",
    "    feature31[subject_id]= mean_all70_160_values\n",
    "    feature32[subject_id]= mean_all200_500_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data31=src.extend_list(feature31)\n",
    "data32=src.extend_list(feature32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the dictionaries to store the results\n",
    "feature33 = {}\n",
    "feature34 = {}\n",
    "\n",
    "# Iterate over each subject in the pre_response2 dictionary\n",
    "for subject_id, values in pre_response2.items():\n",
    "    # Iterate over each channel in the subject's data\n",
    "    for channel_name, channel_values in values.items():\n",
    "        # If the channel is not yet in the feature dictionaries, initialize it\n",
    "        if channel_name not in feature33:\n",
    "            feature33[channel_name] = []\n",
    "        if channel_name not in feature34:\n",
    "            feature34[channel_name] = []\n",
    "        \n",
    "        # Iterate over each event in the channel values\n",
    "        for i in range(len(channel_values)):\n",
    "            # Calculate the mean values for the specified ranges\n",
    "            mean_500_250 = np.mean(channel_values[i][0:126])\n",
    "            mean_250_0 = np.mean(channel_values[i][125:-1])\n",
    "            \n",
    "            # Append the mean values to the corresponding channel's list in the feature dictionaries\n",
    "            feature33[channel_name].append(mean_500_250)\n",
    "            feature34[channel_name].append(mean_250_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data33=feature33\n",
    "data34=feature34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the dictionaries to store the results\n",
    "feature35 = {}\n",
    "feature36 = {}\n",
    "\n",
    "# Iterate over each subject in the pre_response2 dictionary\n",
    "for subject_id, values in post_response2.items():\n",
    "    # Iterate over each channel in the subject's data\n",
    "    for channel_name, channel_values in values.items():\n",
    "        # If the channel is not yet in the feature dictionaries, initialize it\n",
    "        if channel_name not in feature35:\n",
    "            feature35[channel_name] = []\n",
    "        if channel_name not in feature36:\n",
    "            feature36[channel_name] = []\n",
    "        \n",
    "        # Iterate over each event in the channel values\n",
    "        for i in range(len(channel_values)):\n",
    "            # Calculate the mean values for the specified ranges\n",
    "            mean_0_250 = np.mean(channel_values[i][0:126])\n",
    "            mean_250_500 = np.mean(channel_values[i][125:-1])\n",
    "            \n",
    "            # Append the mean values to the corresponding channel's list in the feature dictionaries\n",
    "            feature35[channel_name].append(mean_0_250)\n",
    "            feature36[channel_name].append(mean_250_500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data35=feature35\n",
    "data36=feature36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dicts=[]\n",
    "for feature_index in range(1,37):\n",
    "    feature_corr_key = 'data{}'.format(feature_index)\n",
    "    dicts.append(globals()[feature_corr_key])\n",
    "\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "# Iterate through each dictionary and its index\n",
    "for index, feature_dict in enumerate(dicts, start=1):\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(feature_dict)\n",
    "    # Rename the columns to include the iteration number\n",
    "    df.columns = [f'{index}_{col}' for col in df.columns]\n",
    "    # Concatenate the DataFrame to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, df], axis=1)\n",
    "\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['labels'] = src.extend_list(labels2)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('dataset2_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
